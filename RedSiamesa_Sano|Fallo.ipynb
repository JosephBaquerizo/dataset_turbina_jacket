{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RedSiamesa_Sano|Fallo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWZkyTdqCHazI5LQ3C5zEh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMidbXNZGx7h"
      },
      "source": [
        "from google.colab import drive\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from scipy.io import loadmat\n",
        "from random import randint\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import datetime\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smPHKnZtlQa2",
        "outputId": "0d25e31d-c883-460a-c63b-250f3a9cf237"
      },
      "source": [
        "# Establecemos conexión con Google Drive para acceder a la carpeta de datasets\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPJyJB44lTJa"
      },
      "source": [
        "# Guardamos la dirección de los datasets (experimentos) en una variable para utilizala posteriormente\n",
        "dataset_datos_dir = 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ho0Vc8ElThw"
      },
      "source": [
        "# Obtenemos una lista con todos los nombres de los archivos relacionados a los experimentos realizados\n",
        "dataset_datos_files = [dataset_datos_dir+'/'+filename for filename in listdir(dataset_datos_dir) if isfile(join(dataset_datos_dir, filename))]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dcLn-TjuQFu",
        "outputId": "5b859fdd-dc0f-4975-ccfa-ead126612b1b"
      },
      "source": [
        "dataset_datos_files"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_1_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_5_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_2_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_5_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_3_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_4_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_5_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_1_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_3_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_2_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_10_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_10_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_3_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_4_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_1_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_10_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_2_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_2_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_3_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_4_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_4_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_10_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_1_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_1_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_2_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_6_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_1_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_5_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_3_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_1_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_2_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_3_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_4_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_2_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_5_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_8_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_5_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_1_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_3_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_1_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_4_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_9_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_1_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_7_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_9_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_7_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_2_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_6_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_1_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_1_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_2_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_6_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_8_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_8_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_4_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_3_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_2_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_9_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_6_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_4_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_7_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_8_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/2_5_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_5_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_7_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/1_9_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_2_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_3_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_4_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_3_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_2_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_3_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_4_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_3_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_4_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_1_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_5_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_5_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_4_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_5_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/3_5_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_1_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_2_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_4_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_1_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_1_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_3_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_2_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_5_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_3_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_4_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_3_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_2_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_3_1A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_5_3A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_4_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_4_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_5_2A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_5_05A.mat',\n",
              " 'drive/MyDrive/DATOS_EXPERIMENTALES_JACKET/DATOS/4_2_2A.mat']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUBDeEAmlWNt"
      },
      "source": [
        "# Creamos listas en donde se almacenarán los archivos relacionados a experimentos de fisura y experimentos de perno flojo respectivamente\n",
        "datos_fisura = []\n",
        "datos_pernoflojo = []\n",
        "datos_replica = []\n",
        "datos_normal = []"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAsrAeD9lYa6"
      },
      "source": [
        "# Iteramos los nombres de todos los archivos en la lista general previamente creada\n",
        "for filename in listdir(dataset_datos_dir):\n",
        "  # Obtenemos el nombre del archivo tal cual, sin direcciones previas\n",
        "  if isfile(join(dataset_datos_dir, filename)):\n",
        "    nombre_archivo = filename.split('.')[0]\n",
        "    # El estado de fisura se asocia al número 3 en los nombres de los archivos, si cumple la condición el archivo, guardamos en la lista datos_fisura\n",
        "    if (nombre_archivo.split('_')[0] == '3'):\n",
        "      datos_fisura.append(dataset_datos_dir + '/' + filename)\n",
        "    # El estado de fisura se asocia al número 4 en los nombres de los archivos, si cumple la condición el archivo, guardamos en la lista datos_pernoflojo\n",
        "    elif (nombre_archivo.split('_')[0] == '4'):\n",
        "      datos_pernoflojo.append(dataset_datos_dir + '/' + filename)\n",
        "    # El estado de fisura se asocia al número 4 en los nombres de los archivos, si cumple la condición el archivo, guardamos en la lista datos_pernoflojo\n",
        "    elif (nombre_archivo.split('_')[0] == '1'):\n",
        "      datos_normal.append(dataset_datos_dir + '/' + filename)\n",
        "    # El estado de fisura se asocia al número 4 en los nombres de los archivos, si cumple la condición el archivo, guardamos en la lista datos_pernoflojo\n",
        "    elif (nombre_archivo.split('_')[0] == '2'):\n",
        "      datos_replica.append(dataset_datos_dir + '/' + filename)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTadDt76uhys",
        "outputId": "85fb46ac-7ce5-46a3-9802-58c4f33f6873"
      },
      "source": [
        "len(datos_normal)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTdITG0Hum2b",
        "outputId": "5a1d6841-6352-433f-bd1f-ba0fd6e25142"
      },
      "source": [
        "len(datos_replica)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgfZeadMukpl",
        "outputId": "32f3c50e-366f-4a7c-8f47-7ee15fe9d125"
      },
      "source": [
        "len(datos_fisura)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3CtDQslUs5l",
        "outputId": "81a4800f-bfa8-42c5-e2ac-0581bd151825"
      },
      "source": [
        "len(datos_pernoflojo)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xAtv581mVHS"
      },
      "source": [
        "## Datos Fisura"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIIFsOxHlawU"
      },
      "source": [
        "# Se crean 2 listas, una asociada al número de experimento y otra asociada al nivel de white noise presente durante el experimento \n",
        "numero_experimentos_fisura = []\n",
        "amplitud_experimentos_fisura = []"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOcAdZeOlczH"
      },
      "source": [
        "# Iteramos los nombres de los archivos dentro del conjunto que contiene los datos de fisura\n",
        "for nombre_archivo in datos_fisura:\n",
        "  # Obtenemos el nombre del experimento\n",
        "  nombre_experimento = nombre_archivo.split('/')[4]\n",
        "  # Obtenemos el número del experimento\n",
        "  numero_experimento = nombre_experimento.split('_')[1]\n",
        "  # Obtenemos el nivel de white noise del experimento\n",
        "  amplitud_experimento = nombre_experimento.split('.')[0].split('_')[2].replace('A', '')\n",
        "  # Agregamos el número del experimento a la lista numero_experimentos_fisura creada anteriormente\n",
        "  numero_experimentos_fisura.append(int(numero_experimento))\n",
        "  # Agregamos el nivel de white noise del experimento a la lista amplitud_experimentos_fisura creada anteriormente\n",
        "  if amplitud_experimento == '05':\n",
        "    amplitud_experimento = 0.5\n",
        "    amplitud_experimentos_fisura.append(amplitud_experimento)\n",
        "  else:\n",
        "    amplitud_experimentos_fisura.append(int(amplitud_experimento))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DZbHz6Xn0yK"
      },
      "source": [
        "# Creamos un DataFrame vacío\n",
        "df_fisura = pd.DataFrame()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aruYqONn3kg"
      },
      "source": [
        "# Iteramos por rango la lista datos_fisura\n",
        "for indice in range(len(datos_fisura)):\n",
        "  # Obtenemos la dirección dentro de la iteración correspondiente\n",
        "  direccion = datos_fisura[indice]\n",
        "  # Transformamos el .mat a un formato en el que se pueda transformar a DataFrame\n",
        "  mat = loadmat(direccion)\n",
        "  df = pd.DataFrame(mat['data'])\n",
        "  # Para todos los valores del experimento, creamos columnas en donde sus valores se asocian con el nivel de wn y el # de experimento\n",
        "  df['#_exp'] = numero_experimentos_fisura[indice]\n",
        "  df['amplitud'] = amplitud_experimentos_fisura[indice]\n",
        "  # Colocamos al final del DataFrame df_fisura creado el nuevo DataFrame \n",
        "  df_fisura = pd.concat([df_fisura, df], axis = 0)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "dga169imn5qk",
        "outputId": "4bb4d229-b0a2-4763-c9ed-b732f0adcfcc"
      },
      "source": [
        "# Revisamos el conjunto de datos obtenido para fisura\n",
        "df_fisura.head(8)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>#_exp</th>\n",
              "      <th>amplitud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000132</td>\n",
              "      <td>0.000185</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>0.000299</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000168</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.000344</td>\n",
              "      <td>0.000275</td>\n",
              "      <td>-0.000017</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>-0.000053</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000166</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>0.000279</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>0.000145</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000339</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.000128</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>-0.000027</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000280</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>0.000130</td>\n",
              "      <td>0.000172</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.000161</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>0.000123</td>\n",
              "      <td>0.000269</td>\n",
              "      <td>0.000206</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.000179</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000117</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>-0.000014</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000179</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>0.000249</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.000268</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.000284</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>0.000082</td>\n",
              "      <td>0.000311</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000257</td>\n",
              "      <td>0.000110</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.000120</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000282</td>\n",
              "      <td>0.000277</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>0.000251</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.000342</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>0.000272</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.000302</td>\n",
              "      <td>0.000047</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.000271</td>\n",
              "      <td>0.000168</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>0.000224</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.000247</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>0.000255</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>0.000315</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.000237</td>\n",
              "      <td>-0.000001</td>\n",
              "      <td>0.000334</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>-0.000008</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>-0.000012</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.000176</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000154</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>0.000282</td>\n",
              "      <td>0.000219</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000333</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>0.000145</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>-0.000031</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>0.000237</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>0.000121</td>\n",
              "      <td>0.000280</td>\n",
              "      <td>0.000293</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.000248</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000319</td>\n",
              "      <td>0.000335</td>\n",
              "      <td>-0.000004</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.000201</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3  ...        22        23  #_exp  amplitud\n",
              "0  0.000132  0.000185  0.000122  0.000105  ... -0.000053  0.000018      1       1.0\n",
              "1  0.000092  0.000223  0.000166  0.000127  ...  0.000063 -0.000027      1       1.0\n",
              "2  0.000091  0.000280  0.000188  0.000130  ... -0.000014  0.000020      1       1.0\n",
              "3  0.000179  0.000199  0.000249  0.000102  ...  0.000080  0.000057      1       1.0\n",
              "4  0.000282  0.000277  0.000202  0.000211  ...  0.000105  0.000058      1       1.0\n",
              "5  0.000005  0.000196  0.000247  0.000109  ... -0.000012  0.000006      1       1.0\n",
              "6  0.000176  0.000372  0.000223  0.000154  ...  0.000068 -0.000031      1       1.0\n",
              "7  0.000133  0.000171  0.000187  0.000182  ... -0.000009  0.000095      1       1.0\n",
              "\n",
              "[8 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1IcXccFn7R_",
        "outputId": "4551b7dc-1a5b-411c-fd9d-d6483d7bcd9f"
      },
      "source": [
        "# Si dividimos para 99097 el df_fisura deberíamos obtener 20 experimentos en total\n",
        "len(df_fisura)/99097"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fE42XF0n_0f"
      },
      "source": [
        "### Procesamiento de Datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1GNHnYan-L5"
      },
      "source": [
        "# Establecemos 2 listas, una con los posibles experimentos y otra con los posibles niveles de white noise, lo cual nos ayudará a segmentar los datos\n",
        "experimentos = [1, 2, 3, 4, 5]\n",
        "wns = [0.5, 1, 2, 3]\n",
        "# Creamos una lista en donde pondremos todas las imágenes creadas, segmentadas por experimento y white noise\n",
        "arreglo_imagenes_por_experimento_fisura = []\n",
        "\n",
        "# Iteramos por experimento y por nivel de white noise\n",
        "for i in experimentos:\n",
        "  for j in wns:\n",
        "    # Creamos filtros que nos permiten identificar el experimento por su número asociado y por su nivel de white noise dentro del conjunto de datos\n",
        "    filter1 = df_fisura[\"#_exp\"] == i\n",
        "    filter2 = df_fisura[\"amplitud\"] == j\n",
        "    # Filtramos y deshacemos las filas que no corresponden a la búsqueda\n",
        "    dataset_experimento = df_fisura.where(filter1 & filter2).dropna()\n",
        "    # Creamos arreglo en donde almacenaremos listas de imágenes asociadas a cada columna en un experimento individual\n",
        "    arreglo_matrices_asociadas = []\n",
        "\n",
        "    # En el conjunto filtrado, iteramos por cada columna (de la columna 0 a la 23)\n",
        "    for n in range(24):\n",
        "      # Establecemos un límite inferior y un límite superior\n",
        "      indice_inicio = 0\n",
        "      indice_fin = 256\n",
        "      # Del conjunto filtrado, obtenemos un subconjunto con muestras cada 6 pasos\n",
        "      columna_cada_6 = dataset_experimento[n][::6]\n",
        "      # Creamos una lista en donde guardaremos todas las imágenes creadas dentro de la columna en la que se esta iterando\n",
        "      matrices_columna = []\n",
        "    \n",
        "      # Recorremos en valores de 256 la columna del subconjunto creado para generar las imágenes 16x16 correspondientes \n",
        "      while indice_fin < len(columna_cada_6):\n",
        "        # Guardamos el conjunto de 256 datos dentro de un arreglo\n",
        "        vector = columna_cada_6[indice_inicio:indice_fin]\n",
        "        # Redimensionamos el arreglo (de 1x256 a 16x16)\n",
        "        matriz = vector.to_numpy().reshape((16, 16))\n",
        "        # Agregamos la imagen dentro de la lista matrices_columna\n",
        "        matrices_columna.append(matriz)\n",
        "        # Establecemos el nuevo límite inferior y superior en donde se encontrarán los datos de mi nueva imagen 16x16\n",
        "        indice_inicio = indice_fin\n",
        "        indice_fin = indice_fin + 256\n",
        "      \n",
        "      # Agregamos las listas de imágenes 16x16 asociadas a una columna a arreglo_matrices_asociadas\n",
        "      arreglo_matrices_asociadas.append(matrices_columna)\n",
        "    \n",
        "    # Agregamos arreglo_matrices_asociadas a arreglos_imagenes_por_experimento, en donde tenemos listas de listas asociadas a un experimento individual\n",
        "    arreglo_imagenes_por_experimento_fisura.append(arreglo_matrices_asociadas)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRgjvyFmoUdG",
        "outputId": "7cb6dafb-e48f-4fab-aba1-cf439629f971"
      },
      "source": [
        "len(arreglo_imagenes_por_experimento_fisura)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2b3S5T1oWvh",
        "outputId": "b253ef5a-7529-4b74-c5ff-3c7c782481f6"
      },
      "source": [
        "len(arreglo_imagenes_por_experimento_fisura[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-z6eHRcoY7n",
        "outputId": "7cc77e30-7b9b-4d24-877e-b1f2a337e09b"
      },
      "source": [
        "len(arreglo_imagenes_por_experimento_fisura[0][0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vju0Q8goasH",
        "outputId": "33189304-7b2e-49da-e982-6af8541909c0"
      },
      "source": [
        "arreglo_imagenes_por_experimento_fisura[0][0][0].size"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9Fs9z_Docd0",
        "outputId": "223860a8-4090-46eb-ae2e-035fa3362e69"
      },
      "source": [
        "arreglo_imagenes_por_experimento_fisura[0][0][0].shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYBt3_NWofOD"
      },
      "source": [
        "# Creamos un arreglos general para todas las imágenes de dimensionamiento (24, 16, 16)\n",
        "arreglo_imagenes_fisura = []\n",
        "\n",
        "# Iteramos cada experimento en la lista de experimentos creada previamente\n",
        "for experimento in arreglo_imagenes_por_experimento_fisura:\n",
        "  # Iteramos en un rango de 64 para usarlo posteriormente\n",
        "  for idx in range(64):\n",
        "    # Para cada iteración se crea el arreglo en donde se agregaran las capas correctas de la imagen de dimensionamniento (24, 16, 16)\n",
        "    imagen = []\n",
        "    # Iteramos para cada fila en el experimento (son 24 filas)\n",
        "    for fila in experimento:\n",
        "      # Asociamos por índice los valores de cada lista de imágenes para establecer la relación correcta y agregamos al arreglo creado\n",
        "      capa_imagen = fila[idx]\n",
        "      imagen.append(capa_imagen)\n",
        "\n",
        "    # Transformamos la lista de 24 capas a un arreglo numérico\n",
        "    arreglo_imagenes_fisura.append(np.array(imagen))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9LMzIDRojYQ",
        "outputId": "3b66317c-c99a-4d99-fcbf-0d91e419896c"
      },
      "source": [
        "# Revisamos el número de imágenes creadas\n",
        "len(arreglo_imagenes_fisura)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1280"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejYJ_djvolv0",
        "outputId": "7125bba1-bf2c-45bb-9345-0a495743e566"
      },
      "source": [
        "# Revisamos el dimensionamiento y tipo de un elemento aleatorio en la lista de imágenes\n",
        "indice_imagen_fisura = randint(0, len(arreglo_imagenes_fisura) - 1)\n",
        "arreglo_imagenes_fisura[indice_imagen_fisura].shape, type(arreglo_imagenes_fisura[0]), indice_imagen_fisura"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24, 16, 16), numpy.ndarray, 472)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-VxrL0TmY2x"
      },
      "source": [
        "## Datos Perno Flojo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT4rPuFCmakO"
      },
      "source": [
        "# Se crean 2 listas, una asociada al número de experimento y otra asociada al nivel de white noise presente durante el experimento \n",
        "numero_experimentos_pernoflojo = []\n",
        "amplitud_experimentos_pernoflojo = []"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPK1IcmdoqDX"
      },
      "source": [
        "# Iteramos los nombres de los archivos dentro del conjunto que contiene los datos de fisura\n",
        "for nombre_archivo in datos_pernoflojo:\n",
        "  # Obtenemos el nombre del experimento\n",
        "  nombre_experimento = nombre_archivo.split('/')[4]\n",
        "  # Obtenemos el número del experimento\n",
        "  numero_experimento = nombre_experimento.split('_')[1]\n",
        "  # Obtenemos el nivel de white noise del experimento\n",
        "  amplitud_experimento = nombre_experimento.split('.')[0].split('_')[2].replace('A', '')\n",
        "  # Agregamos el número del experimento a la lista numero_experimentos_pernoflojo creada anteriormente\n",
        "  numero_experimentos_pernoflojo.append(int(numero_experimento))\n",
        "  # Agregamos el nivel de white noise del experimento a la lista amplitud_experimentos_pernoflojo creada anteriormente\n",
        "  if amplitud_experimento == '05':\n",
        "    amplitud_experimento = 0.5\n",
        "    amplitud_experimentos_pernoflojo.append(amplitud_experimento)\n",
        "  else:\n",
        "    amplitud_experimentos_pernoflojo.append(int(amplitud_experimento))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBF3iwl_osHc"
      },
      "source": [
        "# Creamos un DataFrame vacío\n",
        "df_pernoflojo = pd.DataFrame()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhW1w8hFouMg"
      },
      "source": [
        "# Iteramos por rango la lista datos_fisura\n",
        "for indice in range(len(datos_pernoflojo)):\n",
        "  # Obtenemos la dirección dentro de la iteración correspondiente\n",
        "  direccion = datos_pernoflojo[indice]\n",
        "  # Transformamos el .mat a un formato en el que se pueda transformar a DataFrame\n",
        "  mat = loadmat(direccion)\n",
        "  df = pd.DataFrame(mat['data'])\n",
        "  # Para todos los valores del experimento, creamos columnas en donde sus valores se asocian con el nivel de wn y el # de experimento\n",
        "  df['#_exp'] = numero_experimentos_pernoflojo[indice]\n",
        "  df['amplitud'] = amplitud_experimentos_pernoflojo[indice]\n",
        "  # Colocamos al final del DataFrame df_fisura creado el nuevo DataFrame \n",
        "  df_pernoflojo = pd.concat([df_pernoflojo, df], axis = 0)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "qi2vxltWowHB",
        "outputId": "8fcdf009-4b31-4cbc-d38f-5972a183e05d"
      },
      "source": [
        "# Revisamos el conjunto de datos obtenido para pernoflojo\n",
        "df_pernoflojo.head(8)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>#_exp</th>\n",
              "      <th>amplitud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000168</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.000135</td>\n",
              "      <td>0.000201</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.000212</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>0.000257</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000316</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>0.000136</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.000277</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>0.000172</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>0.000272</td>\n",
              "      <td>0.000172</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.000178</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.000128</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.000217</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000309</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000162</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.000154</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.000120</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.000229</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.000260</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>0.000212</td>\n",
              "      <td>0.000178</td>\n",
              "      <td>0.000123</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000130</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000194</td>\n",
              "      <td>0.000244</td>\n",
              "      <td>0.000194</td>\n",
              "      <td>0.000201</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000212</td>\n",
              "      <td>0.000110</td>\n",
              "      <td>0.000266</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>0.000181</td>\n",
              "      <td>0.000040</td>\n",
              "      <td>0.000336</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000287</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>0.000119</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.000216</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>0.000280</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>0.000296</td>\n",
              "      <td>0.000212</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000090</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>0.000247</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.000178</td>\n",
              "      <td>0.000255</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.000216</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.000299</td>\n",
              "      <td>0.000233</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000130</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>0.000212</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.000178</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.000130</td>\n",
              "      <td>0.000260</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>0.000216</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.000329</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>0.000178</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3  ...        22        23  #_exp  amplitud\n",
              "0  0.000168  0.000204  0.000213  0.000135  ...  0.000054  0.000111      1       0.5\n",
              "1  0.000122  0.000277  0.000213  0.000180  ...  0.000089  0.000076      1       0.5\n",
              "2  0.000099  0.000214  0.000128  0.000153  ...  0.000091  0.000092      1       0.5\n",
              "3  0.000150  0.000258  0.000209  0.000163  ...  0.000056  0.000130      1       0.5\n",
              "4  0.000194  0.000244  0.000194  0.000201  ...  0.000064  0.000087      1       0.5\n",
              "5  0.000156  0.000287  0.000188  0.000119  ...  0.000076  0.000091      1       0.5\n",
              "6  0.000150  0.000230  0.000198  0.000239  ...  0.000060  0.000087      1       0.5\n",
              "7  0.000130  0.000230  0.000207  0.000116  ...  0.000079  0.000115      1       0.5\n",
              "\n",
              "[8 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETLCMZD_ox7N",
        "outputId": "db0f3988-b123-431f-fe84-5371517c6528"
      },
      "source": [
        "# Si dividimos para 99097 el df_pernoflojo deberíamos obtener 20 experimentos en total\n",
        "len(df_pernoflojo)/99097"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rlXbOipo1-E"
      },
      "source": [
        "### Procesamiento de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTQPo5t8o4HO"
      },
      "source": [
        "# Establecemos 2 listas, una con los posibles experimentos y otra con los posibles niveles de white noise, lo cual nos ayudará a segmentar los datos\n",
        "experimentos = [1, 2, 3, 4, 5]\n",
        "wns = [0.5, 1, 2, 3]\n",
        "# Creamos una lista en donde pondremos todas las imágenes creadas, segmentadas por experimento y white noise\n",
        "arreglo_imagenes_por_experimento_pernoflojo = []\n",
        "\n",
        "# Iteramos por experimento y por nivel de white noise\n",
        "for i in experimentos:\n",
        "  for j in wns:\n",
        "    # Creamos filtros que nos permiten identificar el experimento por su número asociado y por su nivel de white noise dentro del conjunto de datos\n",
        "    filter1 = df_pernoflojo[\"#_exp\"] == i\n",
        "    filter2 = df_pernoflojo[\"amplitud\"] == j\n",
        "    # Filtramos y deshacemos las filas que no corresponden a la búsqueda\n",
        "    dataset_experimento = df_pernoflojo.where(filter1 & filter2).dropna()\n",
        "    # Creamos arreglo en donde almacenaremos listas de imágenes asociadas a cada columna en un experimento individual\n",
        "    arreglo_matrices_asociadas = []\n",
        "\n",
        "    # En el conjunto filtrado, iteramos por cada columna (de la columna 0 a la 23)\n",
        "    for n in range(24):\n",
        "      # Establecemos un límite inferior y un límite superior\n",
        "      indice_inicio = 0\n",
        "      indice_fin = 256\n",
        "      # Del conjunto filtrado, obtenemos un subconjunto con muestras cada 6 pasos\n",
        "      columna_cada_6 = dataset_experimento[n][::6]\n",
        "      # Creamos una lista en donde guardaremos todas las imágenes creadas dentro de la columna en la que se esta iterando\n",
        "      matrices_columna = []\n",
        "    \n",
        "      # Recorremos en valores de 256 la columna del subconjunto creado para generar las imágenes 16x16 correspondientes \n",
        "      while indice_fin < len(columna_cada_6):\n",
        "        # Guardamos el conjunto de 256 datos dentro de un arreglo\n",
        "        vector = columna_cada_6[indice_inicio:indice_fin]\n",
        "        # Redimensionamos el arreglo (de 1x256 a 16x16)\n",
        "        matriz = vector.to_numpy().reshape((16, 16))\n",
        "        # Agregamos la imagen dentro de la lista matrices_columna\n",
        "        matrices_columna.append(matriz)\n",
        "        # Establecemos el nuevo límite inferior y superior en donde se encontrarán los datos de mi nueva imagen 16x16\n",
        "        indice_inicio = indice_fin\n",
        "        indice_fin = indice_fin + 256\n",
        "      \n",
        "      # Agregamos las listas de imágenes 16x16 asociadas a una columna a arreglo_matrices_asociadas\n",
        "      arreglo_matrices_asociadas.append(matrices_columna)\n",
        "    \n",
        "    # Agregamos arreglo_matrices_asociadas a arreglos_imagenes_por_experimento, en donde tenemos listas de listas asociadas a un experimento individual\n",
        "    arreglo_imagenes_por_experimento_pernoflojo.append(arreglo_matrices_asociadas)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKHzwlHxo64H",
        "outputId": "ead00196-7f77-4416-8116-1ba047326b44"
      },
      "source": [
        "len(arreglo_imagenes_por_experimento_pernoflojo)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ozO33VCo9z5",
        "outputId": "1244219d-c37e-4510-84ee-cf955f70706e"
      },
      "source": [
        "len(arreglo_imagenes_por_experimento_pernoflojo[0])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY2PVH_Lo_XA",
        "outputId": "eff1a763-f378-43ab-baca-b25126f0e6cd"
      },
      "source": [
        "len(arreglo_imagenes_por_experimento_pernoflojo[0][0])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXvg65dcpA1W",
        "outputId": "3b8d82e1-cc3c-4912-da0c-12dab2469ff4"
      },
      "source": [
        "arreglo_imagenes_por_experimento_pernoflojo[0][0][0].size"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssqcTT1IpDvy",
        "outputId": "068e51ce-a1cc-46a7-f8b1-581efb331c45"
      },
      "source": [
        "arreglo_imagenes_por_experimento_pernoflojo[0][0][0].shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33xKAj-BpF5e"
      },
      "source": [
        "# Creamos un arreglos general para todas las imágenes de dimensionamiento (24, 16, 16)\n",
        "arreglo_imagenes_pernoflojo = []\n",
        "\n",
        "# Iteramos cada experimento en la lista de experimentos creada previamente\n",
        "for experimento_i in arreglo_imagenes_por_experimento_pernoflojo:\n",
        "  # Iteramos en un rango de 64 para usarlo posteriormente\n",
        "  for idx in range(64):\n",
        "    # Para cada iteración se crea el arreglo en donde se agregaran las capas correctas de la imagen de dimensionamniento (24, 16, 16)\n",
        "    imagen = []\n",
        "    # Iteramos para cada fila en el experimento (son 24 filas)\n",
        "    for fila in experimento_i:\n",
        "      # Asociamos por índice los valores de cada lista de imágenes para establecer la relación correcta y agregamos al arreglo creado\n",
        "      capa_imagen = fila[idx]\n",
        "      imagen.append(capa_imagen)\n",
        "\n",
        "    # Transformamos la lista de 24 capas a un arreglo numérico\n",
        "    arreglo_imagenes_pernoflojo.append(np.array(imagen))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lskdVB9pGia",
        "outputId": "b1c1b863-6466-464f-a8aa-f692276b4ebb"
      },
      "source": [
        "# Revisamos el número de imágenes creadas\n",
        "len(arreglo_imagenes_pernoflojo)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1280"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzr4-h9GpK_O",
        "outputId": "dfbb542a-5a69-4dbe-e7f0-bb3a36be2746"
      },
      "source": [
        "# Revisamos el dimensionamiento y tipo de un elemento aleatorio en la lista de imágenes\n",
        "indice_imagen_pernoflojo = randint(0, len(arreglo_imagenes_pernoflojo) - 1)\n",
        "arreglo_imagenes_pernoflojo[indice_imagen_pernoflojo].shape, type(arreglo_imagenes_pernoflojo[0]), indice_imagen_pernoflojo"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24, 16, 16), numpy.ndarray, 601)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE0dXKcsnp8q"
      },
      "source": [
        "## Datos Réplica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT_MIw8knrQ0"
      },
      "source": [
        "# Se crean 2 listas, una asociada al número de experimento y otra asociada al nivel de white noise presente durante el experimento \n",
        "numero_experimentos_replica = []\n",
        "amplitud_experimentos_replica = []"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7oxOq6Oq1OF"
      },
      "source": [
        "# Iteramos los nombres de los archivos dentro del conjunto que contiene los datos de fisura\n",
        "for nombre_archivo in datos_replica:\n",
        "  # Obtenemos el nombre del experimento\n",
        "  nombre_experimento = nombre_archivo.split('/')[4]\n",
        "  # Obtenemos el número del experimento\n",
        "  numero_experimento = nombre_experimento.split('_')[1]\n",
        "  # Obtenemos el nivel de white noise del experimento\n",
        "  amplitud_experimento = nombre_experimento.split('.')[0].split('_')[2].replace('A', '')\n",
        "  # Agregamos el número del experimento a la lista numero_experimentos_pernoflojo creada anteriormente\n",
        "  numero_experimentos_replica.append(int(numero_experimento))\n",
        "  # Agregamos el nivel de white noise del experimento a la lista amplitud_experimentos_pernoflojo creada anteriormente\n",
        "  if amplitud_experimento == '05':\n",
        "    amplitud_experimento = 0.5\n",
        "    amplitud_experimentos_replica.append(amplitud_experimento)\n",
        "  else:\n",
        "    amplitud_experimentos_replica.append(int(amplitud_experimento))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVjKkw9vrPqr"
      },
      "source": [
        "# Creamos un DataFrame vacío\n",
        "df_replica = pd.DataFrame()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlsSTnlGrSv8"
      },
      "source": [
        "# Iteramos por rango la lista datos_fisura\n",
        "for indice in range(len(datos_replica)):\n",
        "  # Obtenemos la dirección dentro de la iteración correspondiente\n",
        "  direccion = datos_replica[indice]\n",
        "  # Transformamos el .mat a un formato en el que se pueda transformar a DataFrame\n",
        "  mat = loadmat(direccion)\n",
        "  df = pd.DataFrame(mat['data'])\n",
        "  # Para todos los valores del experimento, creamos columnas en donde sus valores se asocian con el nivel de wn y el # de experimento\n",
        "  df['#_exp'] = numero_experimentos_replica[indice]\n",
        "  df['amplitud'] = amplitud_experimentos_replica[indice]\n",
        "  # Colocamos al final del DataFrame df_fisura creado el nuevo DataFrame \n",
        "  df_replica = pd.concat([df_replica, df], axis = 0)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "hMP34cj0rdWL",
        "outputId": "6ea4a4a4-2612-4846-a236-dc478709e682"
      },
      "source": [
        "# Revisamos el conjunto de datos obtenido para pernoflojo\n",
        "df_replica.head(8)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>#_exp</th>\n",
              "      <th>amplitud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000355</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.000121</td>\n",
              "      <td>-0.000051</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000194</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>0.000252</td>\n",
              "      <td>0.000287</td>\n",
              "      <td>0.000166</td>\n",
              "      <td>0.000288</td>\n",
              "      <td>0.000208</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>-0.000037</td>\n",
              "      <td>0.000383</td>\n",
              "      <td>0.000238</td>\n",
              "      <td>-0.000084</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>0.000121</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>-0.000216</td>\n",
              "      <td>-0.000085</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000346</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.000293</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>0.000226</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000393</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.000349</td>\n",
              "      <td>-0.000049</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.000252</td>\n",
              "      <td>-0.000033</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000136</td>\n",
              "      <td>0.000251</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000227</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.000195</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.000293</td>\n",
              "      <td>0.000143</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>0.000391</td>\n",
              "      <td>0.000449</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.000251</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>0.000326</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000190</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>-0.000056</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.000145</td>\n",
              "      <td>0.000325</td>\n",
              "      <td>0.000121</td>\n",
              "      <td>0.000311</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.000121</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.000378</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000128</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>-0.000052</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000120</td>\n",
              "      <td>0.000354</td>\n",
              "      <td>0.000238</td>\n",
              "      <td>0.000325</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>0.000299</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000269</td>\n",
              "      <td>0.000210</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.000377</td>\n",
              "      <td>0.000047</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>0.000059</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.000090</td>\n",
              "      <td>-0.000045</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000165</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.000249</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>0.000269</td>\n",
              "      <td>0.000243</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.000298</td>\n",
              "      <td>0.000194</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>-0.000044</td>\n",
              "      <td>0.000420</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>-0.000079</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000206</td>\n",
              "      <td>0.000246</td>\n",
              "      <td>0.000181</td>\n",
              "      <td>0.000216</td>\n",
              "      <td>-0.000106</td>\n",
              "      <td>-0.000053</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.000381</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>0.000296</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.000299</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.000121</td>\n",
              "      <td>0.000329</td>\n",
              "      <td>0.000104</td>\n",
              "      <td>0.000272</td>\n",
              "      <td>0.000298</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>0.000117</td>\n",
              "      <td>0.000428</td>\n",
              "      <td>0.000291</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000376</td>\n",
              "      <td>-0.000016</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.000330</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-0.000010</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>0.000437</td>\n",
              "      <td>0.000280</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.000379</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>0.000264</td>\n",
              "      <td>0.000232</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>-0.000024</td>\n",
              "      <td>0.000379</td>\n",
              "      <td>0.000287</td>\n",
              "      <td>-0.000056</td>\n",
              "      <td>0.000125</td>\n",
              "      <td>0.000168</td>\n",
              "      <td>0.000271</td>\n",
              "      <td>0.000246</td>\n",
              "      <td>0.000212</td>\n",
              "      <td>-0.000227</td>\n",
              "      <td>-0.000066</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3  ...        22        23  #_exp  amplitud\n",
              "0  0.000355  0.000146  0.000121 -0.000051  ... -0.000216 -0.000085      1       2.0\n",
              "1  0.000009  0.000346  0.000240  0.000186  ...  0.000252 -0.000033      1       2.0\n",
              "2  0.000136  0.000251  0.000098  0.000175  ...  0.000021 -0.000019      1       2.0\n",
              "3  0.000190  0.000126  0.000240 -0.000056  ...  0.000057 -0.000052      1       2.0\n",
              "4  0.000120  0.000354  0.000238  0.000325  ...  0.000090 -0.000045      1       2.0\n",
              "5  0.000165  0.000087  0.000249  0.000091  ... -0.000106 -0.000053      1       2.0\n",
              "6  0.000381  0.000427  0.000296  0.000122  ...  0.000330  0.000022      1       2.0\n",
              "7 -0.000010  0.000209  0.000437  0.000280  ... -0.000227 -0.000066      1       2.0\n",
              "\n",
              "[8 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifWhstyKrgD0",
        "outputId": "ebbe63b4-4f9a-4c95-fc36-5020953f091c"
      },
      "source": [
        "# Si dividimos para 99097 el df_pernoflojo deberíamos obtener 20 experimentos en total\n",
        "len(df_pernoflojo)/99097"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWsXFaGhrjUh"
      },
      "source": [
        "### Procesamiento de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O77BKLR7rh8c"
      },
      "source": [
        "# Establecemos 2 listas, una con los posibles experimentos y otra con los posibles niveles de white noise, lo cual nos ayudará a segmentar los datos\n",
        "experimentos = [1, 2, 3, 4, 5]\n",
        "wns = [0.5, 1, 2, 3]\n",
        "# Creamos una lista en donde pondremos todas las imágenes creadas, segmentadas por experimento y white noise\n",
        "arreglo_imagenes_por_experimento_replica = []\n",
        "\n",
        "# Iteramos por experimento y por nivel de white noise\n",
        "for i in experimentos:\n",
        "  for j in wns:\n",
        "    # Creamos filtros que nos permiten identificar el experimento por su número asociado y por su nivel de white noise dentro del conjunto de datos\n",
        "    filter1 = df_replica[\"#_exp\"] == i\n",
        "    filter2 = df_replica[\"amplitud\"] == j\n",
        "    # Filtramos y deshacemos las filas que no corresponden a la búsqueda\n",
        "    dataset_experimento = df_replica.where(filter1 & filter2).dropna()\n",
        "    # Creamos arreglo en donde almacenaremos listas de imágenes asociadas a cada columna en un experimento individual\n",
        "    arreglo_matrices_asociadas = []\n",
        "\n",
        "    # En el conjunto filtrado, iteramos por cada columna (de la columna 0 a la 23)\n",
        "    for n in range(24):\n",
        "      # Establecemos un límite inferior y un límite superior\n",
        "      indice_inicio = 0\n",
        "      indice_fin = 256\n",
        "      # Del conjunto filtrado, obtenemos un subconjunto con muestras cada 6 pasos\n",
        "      columna_cada_6 = dataset_experimento[n][::6]\n",
        "      # Creamos una lista en donde guardaremos todas las imágenes creadas dentro de la columna en la que se esta iterando\n",
        "      matrices_columna = []\n",
        "    \n",
        "      # Recorremos en valores de 256 la columna del subconjunto creado para generar las imágenes 16x16 correspondientes \n",
        "      while indice_fin < len(columna_cada_6):\n",
        "        # Guardamos el conjunto de 256 datos dentro de un arreglo\n",
        "        vector = columna_cada_6[indice_inicio:indice_fin]\n",
        "        # Redimensionamos el arreglo (de 1x256 a 16x16)\n",
        "        matriz = vector.to_numpy().reshape((16, 16))\n",
        "        # Agregamos la imagen dentro de la lista matrices_columna\n",
        "        matrices_columna.append(matriz)\n",
        "        # Establecemos el nuevo límite inferior y superior en donde se encontrarán los datos de mi nueva imagen 16x16\n",
        "        indice_inicio = indice_fin\n",
        "        indice_fin = indice_fin + 256\n",
        "      \n",
        "      # Agregamos las listas de imágenes 16x16 asociadas a una columna a arreglo_matrices_asociadas\n",
        "      arreglo_matrices_asociadas.append(matrices_columna)\n",
        "    \n",
        "    # Agregamos arreglo_matrices_asociadas a arreglos_imagenes_por_experimento, en donde tenemos listas de listas asociadas a un experimento individual\n",
        "    arreglo_imagenes_por_experimento_replica.append(arreglo_matrices_asociadas)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osa7BMGLrxcI",
        "outputId": "2babb486-4369-40e0-b57b-4e881cc55e83"
      },
      "source": [
        "len(arreglo_imagenes_por_experimento_replica)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6FdtEv6rzEa",
        "outputId": "93c3d0c9-5909-4c54-f20c-afccded018c6"
      },
      "source": [
        "len(arreglo_imagenes_por_experimento_replica[0])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W-83zSqr0cB",
        "outputId": "6054fe68-4d44-4728-c6a0-35eebb89c212"
      },
      "source": [
        "len(arreglo_imagenes_por_experimento_replica[0][0])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re8T9TI_r3ZS",
        "outputId": "89585f4f-e59b-4bc6-9630-21f983f719a5"
      },
      "source": [
        "arreglo_imagenes_por_experimento_replica[0][0][0].size"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3dAA2N-r3xK",
        "outputId": "c9479f8c-09ba-43ce-8fdd-b85cd4d3a3f2"
      },
      "source": [
        "arreglo_imagenes_por_experimento_replica[0][0][0].shape"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPRdP8Fcr5qJ"
      },
      "source": [
        "# Creamos un arreglos general para todas las imágenes de dimensionamiento (24, 16, 16)\n",
        "arreglo_imagenes_replica = []\n",
        "\n",
        "# Iteramos cada experimento en la lista de experimentos creada previamente\n",
        "for experimento_i in arreglo_imagenes_por_experimento_replica:\n",
        "  # Iteramos en un rango de 64 para usarlo posteriormente\n",
        "  for idx in range(64):\n",
        "    # Para cada iteración se crea el arreglo en donde se agregaran las capas correctas de la imagen de dimensionamniento (24, 16, 16)\n",
        "    imagen = []\n",
        "    # Iteramos para cada fila en el experimento (son 24 filas)\n",
        "    for fila in experimento_i:\n",
        "      # Asociamos por índice los valores de cada lista de imágenes para establecer la relación correcta y agregamos al arreglo creado\n",
        "      capa_imagen = fila[idx]\n",
        "      imagen.append(capa_imagen)\n",
        "\n",
        "    # Transformamos la lista de 24 capas a un arreglo numérico\n",
        "    arreglo_imagenes_replica.append(np.array(imagen))"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ0EuskOr_bj",
        "outputId": "4f0339f0-61e7-468c-87f8-7692610817df"
      },
      "source": [
        "# Revisamos el número de imágenes creadas\n",
        "len(arreglo_imagenes_replica)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1280"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5L2UWaPsIPF",
        "outputId": "73a59a1d-43af-43f5-82dc-3253a3e65bc8"
      },
      "source": [
        "# Revisamos el dimensionamiento y tipo de un elemento aleatorio en la lista de imágenes\n",
        "indice_imagen_replica = randint(0, len(arreglo_imagenes_replica) - 1)\n",
        "arreglo_imagenes_replica[indice_imagen_replica].shape, type(arreglo_imagenes_replica[0]), indice_imagen_replica"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24, 16, 16), numpy.ndarray, 137)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Goxx4h89vQEG"
      },
      "source": [
        "## Datos Normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99nedCUovSmm"
      },
      "source": [
        "# Se crean 2 listas, una asociada al número de experimento y otra asociada al nivel de white noise presente durante el experimento \n",
        "numero_experimentos_normal = []\n",
        "amplitud_experimentos_normal = []"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeUOiNrevajt"
      },
      "source": [
        "# Iteramos los nombres de los archivos dentro del conjunto que contiene los datos de fisura\n",
        "for nombre_archivo in datos_normal:\n",
        "  # Obtenemos el nombre del experimento\n",
        "  nombre_experimento = nombre_archivo.split('/')[4]\n",
        "  # Obtenemos el número del experimento\n",
        "  numero_experimento = nombre_experimento.split('_')[1]\n",
        "  # Obtenemos el nivel de white noise del experimento\n",
        "  amplitud_experimento = nombre_experimento.split('.')[0].split('_')[2].replace('A', '')\n",
        "  # Agregamos el número del experimento a la lista numero_experimentos_pernoflojo creada anteriormente\n",
        "  numero_experimentos_normal.append(int(numero_experimento))\n",
        "  # Agregamos el nivel de white noise del experimento a la lista amplitud_experimentos_pernoflojo creada anteriormente\n",
        "  if amplitud_experimento == '05':\n",
        "    amplitud_experimento = 0.5\n",
        "    amplitud_experimentos_normal.append(amplitud_experimento)\n",
        "  else:\n",
        "    amplitud_experimentos_normal.append(int(amplitud_experimento))"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-t4CfbGpvfiw"
      },
      "source": [
        "# Creamos un DataFrame vacío\n",
        "df_normal = pd.DataFrame()"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnlaZc80vf-1"
      },
      "source": [
        "# Iteramos por rango la lista datos_fisura\n",
        "for indice in range(len(datos_normal)):\n",
        "  # Obtenemos la dirección dentro de la iteración correspondiente\n",
        "  direccion = datos_normal[indice]\n",
        "  # Transformamos el .mat a un formato en el que se pueda transformar a DataFrame\n",
        "  mat = loadmat(direccion)\n",
        "  df = pd.DataFrame(mat['data'])\n",
        "  # Para todos los valores del experimento, creamos columnas en donde sus valores se asocian con el nivel de wn y el # de experimento\n",
        "  df['#_exp'] = numero_experimentos_normal[indice]\n",
        "  df['amplitud'] = amplitud_experimentos_normal[indice]\n",
        "  # Colocamos al final del DataFrame df_fisura creado el nuevo DataFrame \n",
        "  df_normal = pd.concat([df_normal, df], axis = 0)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AieMZxcsvjBV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "72e0916d-78df-4c84-95e3-4591b9dd868d"
      },
      "source": [
        "# Revisamos el conjunto de datos obtenido para pernoflojo\n",
        "df_normal.head(8)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>#_exp</th>\n",
              "      <th>amplitud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000164</td>\n",
              "      <td>0.000316</td>\n",
              "      <td>0.000257</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>0.000217</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.000316</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.000274</td>\n",
              "      <td>0.000244</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000348</td>\n",
              "      <td>0.000246</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>0.000195</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>-0.000012</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000145</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.000144</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>0.000210</td>\n",
              "      <td>0.000329</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000273</td>\n",
              "      <td>0.000263</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>0.000392</td>\n",
              "      <td>0.000279</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.000273</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000104</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.000263</td>\n",
              "      <td>0.000266</td>\n",
              "      <td>0.000179</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.000334</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000264</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.000195</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.000383</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>0.000172</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000182</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>0.000243</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.000252</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.000348</td>\n",
              "      <td>0.000162</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000351</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>0.000210</td>\n",
              "      <td>0.000136</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>-0.000037</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.000195</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.000298</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>0.000289</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000132</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000353</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000185</td>\n",
              "      <td>0.000117</td>\n",
              "      <td>0.000130</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>-0.000029</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000142</td>\n",
              "      <td>0.000305</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.000224</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>0.000272</td>\n",
              "      <td>0.000254</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000403</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.000248</td>\n",
              "      <td>0.000136</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>-0.000062</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000227</td>\n",
              "      <td>0.000229</td>\n",
              "      <td>0.000136</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.000335</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.000237</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000417</td>\n",
              "      <td>0.000219</td>\n",
              "      <td>-0.000008</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.000136</td>\n",
              "      <td>0.000185</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000244</td>\n",
              "      <td>0.000208</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.000181</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.000144</td>\n",
              "      <td>0.000282</td>\n",
              "      <td>0.000162</td>\n",
              "      <td>0.000263</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>0.000178</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.000130</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>-0.000007</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3  ...        22        23  #_exp  amplitud\n",
              "0  0.000164  0.000316  0.000257  0.000163  ...  0.000086 -0.000012      1       0.5\n",
              "1  0.000145  0.000225  0.000230  0.000150  ...  0.000104  0.000011      1       0.5\n",
              "2  0.000192  0.000263  0.000266  0.000179  ...  0.000084 -0.000006      1       0.5\n",
              "3  0.000182  0.000258  0.000243  0.000177  ...  0.000068 -0.000037      1       0.5\n",
              "4  0.000150  0.000261  0.000177  0.000153  ...  0.000027 -0.000029      1       0.5\n",
              "5  0.000142  0.000305  0.000240  0.000157  ...  0.000062 -0.000062      1       0.5\n",
              "6  0.000137  0.000227  0.000229  0.000136  ...  0.000022  0.000015      1       0.5\n",
              "7  0.000138  0.000244  0.000208  0.000149  ...  0.000056 -0.000007      1       0.5\n",
              "\n",
              "[8 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHkkdBl6vlvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fadf6aa-4fff-4692-dc96-4a62e2df32c0"
      },
      "source": [
        "# Si dividimos para 99097 el df_pernoflojo deberíamos obtener 20 experimentos en total NO SABEMOS TODAVIA\n",
        "len(df_normal)/99097"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGEgnmPDvqPZ"
      },
      "source": [
        "### Procesamiento de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7CJ9lRRvpRE"
      },
      "source": [
        "# Establecemos 2 listas, una con los posibles experimentos y otra con los posibles niveles de white noise, lo cual nos ayudará a segmentar los datos\n",
        "experimentosn = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "wns = [0.5, 1, 2, 3]\n",
        "# Creamos una lista en donde pondremos todas las imágenes creadas, segmentadas por experimento y white noise\n",
        "arreglo_imagenes_por_experimento_normal = []\n",
        "\n",
        "# Iteramos por experimento y por nivel de white noise\n",
        "for i in experimentosn:\n",
        "  for j in wns:\n",
        "    # Creamos filtros que nos permiten identificar el experimento por su número asociado y por su nivel de white noise dentro del conjunto de datos\n",
        "    filter1 = df_normal[\"#_exp\"] == i\n",
        "    filter2 = df_normal[\"amplitud\"] == j\n",
        "    # Filtramos y deshacemos las filas que no corresponden a la búsqueda\n",
        "    dataset_experimento = df_normal.where(filter1 & filter2).dropna()\n",
        "    # Creamos arreglo en donde almacenaremos listas de imágenes asociadas a cada columna en un experimento individual\n",
        "    arreglo_matrices_asociadas = []\n",
        "\n",
        "    # En el conjunto filtrado, iteramos por cada columna (de la columna 0 a la 23)\n",
        "    for n in range(24):\n",
        "      # Establecemos un límite inferior y un límite superior\n",
        "      indice_inicio = 0\n",
        "      indice_fin = 256\n",
        "      # Del conjunto filtrado, obtenemos un subconjunto con muestras cada 6 pasos\n",
        "      columna_cada_6 = dataset_experimento[n][::6]\n",
        "      # Creamos una lista en donde guardaremos todas las imágenes creadas dentro de la columna en la que se esta iterando\n",
        "      matrices_columna = []\n",
        "    \n",
        "      # Recorremos en valores de 256 la columna del subconjunto creado para generar las imágenes 16x16 correspondientes \n",
        "      while indice_fin < len(columna_cada_6):\n",
        "        # Guardamos el conjunto de 256 datos dentro de un arreglo\n",
        "        vector = columna_cada_6[indice_inicio:indice_fin]\n",
        "        # Redimensionamos el arreglo (de 1x256 a 16x16)\n",
        "        matriz = vector.to_numpy().reshape((16, 16))\n",
        "        # Agregamos la imagen dentro de la lista matrices_columna\n",
        "        matrices_columna.append(matriz)\n",
        "        # Establecemos el nuevo límite inferior y superior en donde se encontrarán los datos de mi nueva imagen 16x16\n",
        "        indice_inicio = indice_fin\n",
        "        indice_fin = indice_fin + 256\n",
        "      \n",
        "      # Agregamos las listas de imágenes 16x16 asociadas a una columna a arreglo_matrices_asociadas\n",
        "      arreglo_matrices_asociadas.append(matrices_columna)\n",
        "    \n",
        "    # Agregamos arreglo_matrices_asociadas a arreglos_imagenes_por_experimento, en donde tenemos listas de listas asociadas a un experimento individual\n",
        "    arreglo_imagenes_por_experimento_normal.append(arreglo_matrices_asociadas)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILYkKKuuv4Jm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "382d75b8-8099-444b-9e9b-b631953a6dcb"
      },
      "source": [
        "len(arreglo_imagenes_por_experimento_normal)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b5vkJ3Lv70b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a70acb0-cf67-47a4-cf91-43985472b354"
      },
      "source": [
        "len(arreglo_imagenes_por_experimento_normal[0])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCw4Iz-fwAN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb47cd8-c977-4f17-d1d6-a8a0fa3892ae"
      },
      "source": [
        "len(arreglo_imagenes_por_experimento_normal[0][0])"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5liqHfLDwDrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b926098-b7fa-40f3-d389-e5e6e1de9517"
      },
      "source": [
        "arreglo_imagenes_por_experimento_normal[0][0][0].size"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4zYoJsuwIUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87d53f9a-76c7-4b05-ff8a-8965d4fc6747"
      },
      "source": [
        "arreglo_imagenes_por_experimento_normal[0][0][0].shape"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXx7TW-NwPvQ"
      },
      "source": [
        "# Creamos un arreglos general para todas las imágenes de dimensionamiento (24, 16, 16)\n",
        "arreglo_imagenes_normal = []\n",
        "\n",
        "# Iteramos cada experimento en la lista de experimentos creada previamente\n",
        "for experimento_i in arreglo_imagenes_por_experimento_normal:\n",
        "  # Iteramos en un rango de 64 para usarlo posteriormente\n",
        "  for idx in range(64):\n",
        "    # Para cada iteración se crea el arreglo en donde se agregaran las capas correctas de la imagen de dimensionamniento (24, 16, 16)\n",
        "    imagen = []\n",
        "    # Iteramos para cada fila en el experimento (son 24 filas)\n",
        "    for fila in experimento_i:\n",
        "      # Asociamos por índice los valores de cada lista de imágenes para establecer la relación correcta y agregamos al arreglo creado\n",
        "      capa_imagen = fila[idx]\n",
        "      imagen.append(capa_imagen)\n",
        "\n",
        "    # Transformamos la lista de 24 capas a un arreglo numérico\n",
        "    arreglo_imagenes_normal.append(np.array(imagen))"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-mZ8iA1wQbA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d1d0d6-b3c5-482c-b14c-060b75bc5ed1"
      },
      "source": [
        "# Revisamos el número de imágenes creadas\n",
        "len(arreglo_imagenes_normal)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2560"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiJ-1s6IwT_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b787eb-b19f-4217-9c40-c868cd8e92f1"
      },
      "source": [
        "# Revisamos el dimensionamiento y tipo de un elemento aleatorio en la lista de imágenes\n",
        "indice_imagen_normal = randint(0, len(arreglo_imagenes_normal) - 1)\n",
        "arreglo_imagenes_normal[indice_imagen_normal].shape, type(arreglo_imagenes_normal[0]), indice_imagen_normal"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24, 16, 16), numpy.ndarray, 95)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGTG7mXGXxP2"
      },
      "source": [
        "# Creamos los conjuntos generales de imágenes asociados a fallo (fisura y pernoflojo) y sano (normal y réplica)\n",
        "arreglo_imagenes_fallos = arreglo_imagenes_fisura + arreglo_imagenes_pernoflojo\n",
        "arreglo_imagenes_sano = arreglo_imagenes_normal + arreglo_imagenes_replica"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V19cuVIuYaZA",
        "outputId": "4300ec81-309a-48ce-ac51-d97cc17acda8"
      },
      "source": [
        "len(arreglo_imagenes_fallos), len(arreglo_imagenes_sano)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2560, 3840)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI-70EiPZO4y",
        "outputId": "9ca00880-44cd-46dd-d656-54171f9bbf1e"
      },
      "source": [
        "len(arreglo_imagenes_sano) - len(arreglo_imagenes_fallos)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1280"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf73b2w5a0TK"
      },
      "source": [
        "arreglo_imagenes_fallos_auxiliar = []\n",
        "\n",
        "for i in range(len(arreglo_imagenes_sano) - len(arreglo_imagenes_fallos)):\n",
        "  arreglo_imagenes_fallos_auxiliar.append(random.choice(arreglo_imagenes_fallos))"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VgRCumzbLec",
        "outputId": "dbf31e1d-3649-4011-c910-43e4dbe3d5ca"
      },
      "source": [
        "len(arreglo_imagenes_fallos_auxiliar)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1280"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW0crKqCsaCv"
      },
      "source": [
        "## Red Neuronal Siamesa\n",
        "Tenemos:\n",
        "- arreglo_imagenes_fallos\n",
        "- arreglo_imagenes_sano"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_YikLKWsb-R"
      },
      "source": [
        "def crear_pares(arreglo_imagenes_sano, arreglo_imagenes_fallos, arreglo_imagenes_fallos_auxiliar):\n",
        "  \"\"\"\n",
        "  Función empleada para crear los pares de imágenes que se utilizan\n",
        "  para el entrenamiento de la SNN.\n",
        "  \"\"\"\n",
        "  print(\"Creando pares...\")\n",
        "  # Creamos arreglo en donde se insertarán los pares (positivos y negativos)\n",
        "  pares_img = []\n",
        "  # Creamos arreglo en donde se insertarán la clasificación de los pares (similares = 1, no similares = 0)\n",
        "  pares_label = []\n",
        "\n",
        "  # Recorremos el arreglo de imágenes de fisura y por cada imagen recorrida creamos un par positivo y uno negativo\n",
        "  for imagen_sano in arreglo_imagenes_sano:\n",
        "\n",
        "    # La imagen positiva será un dato aleatorio perteneciente al arreglo de la misma clase (fisura)\n",
        "    imagen_positiva_sano = random.choice(arreglo_imagenes_sano)\n",
        "    # La imagen negativa será un dato aleatorio perteneciente al arreglo de la otra clase (perno flojo)\n",
        "    imagen_negativa_sano = random.choice(arreglo_imagenes_fallos)\n",
        "\n",
        "    # Agregamos el par de imagenes positivas al arreglo de pares de imagenes\n",
        "    pares_img.append([imagen_sano, imagen_positiva_sano])\n",
        "    # Agregamos el label correspondiente a un par de imágenes positivas (1)\n",
        "    pares_label.append([1])\n",
        "\n",
        "    # Agregamos el par de imagenes negativas al arreglo de pares de imagenes\n",
        "    pares_img.append([imagen_sano, imagen_negativa_sano])\n",
        "    # Agregamos el label correspondiente a un par de imágenes negativas (0)\n",
        "    pares_label.append([0])\n",
        "\n",
        "  # Recorremos el arreglo de imágenes de fisura y por cada imagen recorrida creamos un par positivo y uno negativo\n",
        "  for imagen_fallos in arreglo_imagenes_fallos:\n",
        "\n",
        "    # La imagen positiva será un dato aleatorio perteneciente al arreglo de la misma clase (perno flojo)\n",
        "    imagen_positiva_fallos = random.choice(arreglo_imagenes_fallos)\n",
        "    # La imagen negativa será un dato aleatorio perteneciente al arreglo de la otra clase (fisura)\n",
        "    imagen_negativa_fallos = random.choice(arreglo_imagenes_sano)\n",
        "\n",
        "    # Agregamos el par de imagenes positivas al arreglo de pares de imagenes\n",
        "    pares_img.append([imagen_fallos, imagen_positiva_fallos])\n",
        "    # Agregamos el label correspondiente a un par de imágenes positivas (1)\n",
        "    pares_label.append([1])\n",
        "\n",
        "    # Agregamos el par de imagenes negativas al arreglo de pares de imagenes\n",
        "    pares_img.append([imagen_fallos, imagen_negativa_fallos])\n",
        "    # Agregamos el label correspondiente a un par de imágenes negativas (0)\n",
        "    pares_label.append([0])\n",
        "  \n",
        "  for imagen_fallos_auxiliar in arreglo_imagenes_fallos_auxiliar:\n",
        "\n",
        "    # La imagen positiva será un dato aleatorio perteneciente al arreglo de la misma clase (perno flojo)\n",
        "    imagen_positiva_fallos_auxiliar = random.choice(arreglo_imagenes_fallos)\n",
        "    # La imagen negativa será un dato aleatorio perteneciente al arreglo de la otra clase (fisura)\n",
        "    imagen_negativa_fallos_auxiliar = random.choice(arreglo_imagenes_sano)\n",
        "\n",
        "    # Agregamos el par de imagenes positivas al arreglo de pares de imagenes\n",
        "    pares_img.append([imagen_fallos_auxiliar, imagen_positiva_fallos_auxiliar])\n",
        "    # Agregamos el label correspondiente a un par de imágenes positivas (1)\n",
        "    pares_label.append([1])\n",
        "\n",
        "    # Agregamos el par de imagenes negativas al arreglo de pares de imagenes\n",
        "    pares_img.append([imagen_fallos_auxiliar, imagen_negativa_fallos_auxiliar])\n",
        "    # Agregamos el label correspondiente a un par de imágenes negativas (0)\n",
        "    pares_label.append([0])\n",
        "\n",
        "  print(\"Pares creados.\")\n",
        "\n",
        "  return (np.array(pares_img), np.array(pares_label))"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJtFVGMwcNpq",
        "outputId": "987ae571-353d-470f-e2f1-21b6781b3136"
      },
      "source": [
        "(pares, labels) = crear_pares(arreglo_imagenes_sano, arreglo_imagenes_fallos, arreglo_imagenes_fallos_auxiliar)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creando pares...\n",
            "Pares creados.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1q0V9H-caQA",
        "outputId": "b08add0e-8b11-44d7-fd06-26f35d7233ed"
      },
      "source": [
        "len(pares), len(labels)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15360, 15360)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yj4l8aWceHA"
      },
      "source": [
        "# 3840 para sano total\n",
        "# 2560 para fallo + 1280 de fallo auxiliar = 3840 para fallo total\n",
        "# 3840 sano total + 3840 fallo total = 7680 imagenes totales\n",
        "# 2 pares por imagens (un par positivo + un par negativo = 2 pares)\n",
        "# 7680 imagenes totales x 2 pares por cada imagen = 15360 pares de imágenes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viQ5XeNBeXXe"
      },
      "source": [
        "### Segmentación en entrenamiento, validación y prueba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le0mFa4OebUf"
      },
      "source": [
        "np.random.seed(300)\n",
        "\n",
        "pares_aleatorizados = pares\n",
        "np.random.shuffle(pares_aleatorizados)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rylgTjI6eh5p"
      },
      "source": [
        "np.random.seed(300)\n",
        "\n",
        "labels_aleatorizados = labels\n",
        "np.random.shuffle(labels_aleatorizados)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht6ud_5Jej8e"
      },
      "source": [
        "# Obtenemos el 80%, 70%, 15% y 10% de los valores totales\n",
        "porcentaje_80 = int(len(pares_aleatorizados)*0.8)\n",
        "porcentaje_70 = int(len(pares_aleatorizados)*0.7)\n",
        "porcentaje_15 = int(len(pares_aleatorizados)*0.15)\n",
        "porcentaje_10 = int(len(pares_aleatorizados)*0.10)\n",
        "\n",
        "porcentaje_80, porcentaje_70, porcentaje_15, porcentaje_10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc4T_LveenxM"
      },
      "source": [
        "### Creación de SNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCjUSotuemD_"
      },
      "source": [
        "def euclidean_distance(vectors):\n",
        "  \n",
        "  # Descomprimimos los vectores en listas separadas\n",
        "  (featsA, featsB) = vectors\n",
        "\n",
        "  # Realizamos la suma de las diferenias cuadradas entre vectores\n",
        "  sumSquared = K.sum(K.square(featsA - featsB), axis=1, keepdims=True)\n",
        "\n",
        "  # Retornamos la distancia euclidiana\n",
        "  return K.sqrt(K.maximum(sumSquared, K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3NLzDBaethS"
      },
      "source": [
        "def graficar_curva_perdidas(history):\n",
        "  \"\"\"\n",
        "  Función utilizada para visualizar la historia de nuestro modelo.\n",
        "  Incluye pérdidas y precisión a lo largo de cada iteración.\n",
        "  \"\"\"\n",
        "  # Obtenemos las pérdidas de los conjuntos de entrenamiento y validación\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  # Obtenemos la precisión de los conjuntos de entrenamiento y validación\n",
        "  accuracy = history.history['accuracy']\n",
        "  val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  # Determinamos las iteraciones que hubo durante el entrenamiento\n",
        "  epochs = range(len(history.history['loss']))\n",
        "\n",
        "  # Graficamos las pérdidas de ambos conjuntos a lo largo de las iteraciones\n",
        "  plt.plot(epochs, loss, label='training_loss')\n",
        "  plt.plot(epochs, val_loss, label='val_loss')\n",
        "  plt.title('loss')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.legend()\n",
        "\n",
        "  # Graficamos la precisión de ambos conjuntos a lo largo de las iteraciones\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
        "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
        "  plt.title('accuracy')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvkSeAa3ezMf"
      },
      "source": [
        "Modelo Experimental #1\n",
        "\n",
        "- entrenamiento = 80%\n",
        "- validación = 10%\n",
        "- prueba = 10%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YmO5Bv6e16G"
      },
      "source": [
        "# Definimos el conjunto de entrenamiento\n",
        "me1_pares_entrenamiento = pares_aleatorizados[:porcentaje_80]\n",
        "me1_labels_entrenamiento = labels_aleatorizados[:porcentaje_80]\n",
        "\n",
        "# Definimos el conjunto de validación\n",
        "me1_pares_validacion = pares_aleatorizados[porcentaje_80:porcentaje_80+porcentaje_10]\n",
        "me1_labels_validacion = labels_aleatorizados[porcentaje_80:porcentaje_80+porcentaje_10]\n",
        "\n",
        "# Definimos el conjunto de prueba\n",
        "me1_pares_prueba = pares_aleatorizados[porcentaje_80+porcentaje_10:]\n",
        "me1_labels_prueba = labels_aleatorizados[porcentaje_80+porcentaje_10:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osXjMr-he4Sl"
      },
      "source": [
        "\n",
        "# Creamos función que retorna la estructura de la red convolucional que se va a utilizar\n",
        "def build_siamese_model_1(input_shape):\n",
        "  model = tf.keras.Sequential([\n",
        "                                tf.keras.layers.Conv2D(filters=64,\n",
        "                                                        kernel_size=10,\n",
        "                                                        activation='relu',\n",
        "                                                        padding='same',\n",
        "                                                        input_shape=input_shape),\n",
        "                                tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "                                tf.keras.layers.Conv2D(filters=128,\n",
        "                                                        kernel_size=7,\n",
        "                                                        activation='relu',\n",
        "                                                        padding='same'),\n",
        "                                tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "                                tf.keras.layers.Conv2D(filters=128,\n",
        "                                                        kernel_size=4,\n",
        "                                                        activation='relu',\n",
        "                                                        padding='same'),\n",
        "                                tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "                                tf.keras.layers.Conv2D(filters=256,\n",
        "                                                        kernel_size=4,\n",
        "                                                        activation='relu',\n",
        "                                                        padding='same'),\n",
        "                                tf.keras.layers.Flatten(),\n",
        "                                tf.keras.layers.Dense(4960)\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBPVRYXke6aP"
      },
      "source": [
        "tf.random.set_seed(100)\n",
        "\n",
        "# Creamos el modelo general\n",
        "IMG_SHAPE = (24, 16, 16)\n",
        "\n",
        "# Definimos los inputs (imágenes) que ingresan a cada CNN\n",
        "imgA = tf.keras.Input(IMG_SHAPE)\n",
        "imgB = tf.keras.Input(IMG_SHAPE)\n",
        "\n",
        "# Creamos el featureExtractor, el cual obtiene las características principales de las imágenes ingresadas y las muestra en un vector\n",
        "featureExtractor = build_siamese_model_1(input_shape=IMG_SHAPE)\n",
        "\n",
        "# Ingresamos cada imagen al featureExtractor\n",
        "featsA = featureExtractor(imgA)\n",
        "featsB = featureExtractor(imgB)\n",
        "\n",
        "# Creamos una capa Lambda (que aplica una función sobre los datos) para obtener la distancia euclidiana\n",
        "distancia = tf.keras.layers.Lambda(euclidean_distance)([featsA, featsB])\n",
        "\n",
        "# Creamos la última capa asociada a la función de activación sigmoide\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(distancia)\n",
        "\n",
        "# Establecemos el modelo general\n",
        "model_1 = tf.keras.Model(inputs=[imgA, imgB], outputs=outputs)\n",
        "\n",
        "# Empleamos el loss como binary_crossentropy porque solo queremos saber si las imágenes son similares o no (no queremos clasificarlas)\n",
        "model_1.compile(loss='binary_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Hacemos el fit del modelo\n",
        "history_1 = model_1.fit(\n",
        "    [me1_pares_entrenamiento[:, 0], me1_pares_entrenamiento[:, 1]], me1_labels_entrenamiento[:],\n",
        "     validation_data=([me1_pares_validacion[:, 0], me1_pares_validacion[:, 1]], me1_labels_validacion[:]),\n",
        "     batch_size=32,\n",
        "     epochs=5\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P25jw1aVev8H"
      },
      "source": [
        "graficar_curva_perdidas(history_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhrbTJ9cfJ7L"
      },
      "source": [
        "\n",
        "Modelo Experimental #2\n",
        "\n",
        "- entrenamiento = 80%\n",
        "- validación = 15%\n",
        "- prueba = 5%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8AS8XJzfL64"
      },
      "source": [
        "# Definimos el conjunto de entrenamiento\n",
        "me2_pares_entrenamiento = pares_aleatorizados[:porcentaje_80]\n",
        "me2_labels_entrenamiento = labels_aleatorizados[:porcentaje_80]\n",
        "\n",
        "# Definimos el conjunto de validación\n",
        "me2_pares_validacion = pares_aleatorizados[porcentaje_80:porcentaje_80+porcentaje_15]\n",
        "me2_labels_validacion = labels_aleatorizados[porcentaje_80:porcentaje_80+porcentaje_15]\n",
        "\n",
        "# Definimos el conjunto de prueba\n",
        "me2_pares_prueba = pares_aleatorizados[porcentaje_80+porcentaje_15:]\n",
        "me2_labels_prueba = labels_aleatorizados[porcentaje_80+porcentaje_15:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfiTPwkefOEV"
      },
      "source": [
        "def build_siamese_model_2(input_shape):\n",
        "  model = tf.keras.Sequential([\n",
        "                                tf.keras.layers.Conv2D(filters=64,\n",
        "                                                        kernel_size=10,\n",
        "                                                        activation='relu',\n",
        "                                                        padding='same',\n",
        "                                                        input_shape=input_shape),\n",
        "                                tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "                                tf.keras.layers.Conv2D(filters=128,\n",
        "                                                        kernel_size=7,\n",
        "                                                        activation='relu',\n",
        "                                                        padding='same'),\n",
        "                                tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "                                tf.keras.layers.Conv2D(filters=128,\n",
        "                                                        kernel_size=4,\n",
        "                                                        activation='relu',\n",
        "                                                        padding='same'),\n",
        "                                tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "                                tf.keras.layers.Conv2D(filters=256,\n",
        "                                                        kernel_size=4,\n",
        "                                                        activation='relu',\n",
        "                                                        padding='same'),\n",
        "                                tf.keras.layers.Flatten(),\n",
        "                                tf.keras.layers.Dense(4960)\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emVS8P3sfP-r"
      },
      "source": [
        "tf.random.set_seed(100)\n",
        "\n",
        "IMG_SHAPE = (24, 16, 16)\n",
        "\n",
        "imgA = tf.keras.Input(IMG_SHAPE)\n",
        "imgB = tf.keras.Input(IMG_SHAPE)\n",
        "\n",
        "featureExtractor = build_siamese_model_2(input_shape=IMG_SHAPE)\n",
        "\n",
        "featsA = featureExtractor(imgA)\n",
        "featsB = featureExtractor(imgB)\n",
        "\n",
        "distancia = tf.keras.layers.Lambda(euclidean_distance)([featsA, featsB])\n",
        "\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(distancia)\n",
        "\n",
        "model_2 = tf.keras.Model(inputs=[imgA, imgB], outputs=outputs)\n",
        "\n",
        "model_2.compile(loss='binary_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "history_2 = model_2.fit(\n",
        "    [me2_pares_entrenamiento[:, 0], me2_pares_entrenamiento[:, 1]], me2_labels_entrenamiento[:],\n",
        "     validation_data=([me2_pares_validacion[:, 0], me2_pares_validacion[:, 1]], me2_labels_validacion[:]),\n",
        "     batch_size=32,\n",
        "     epochs=5\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMdWS39wfSFA"
      },
      "source": [
        "graficar_curva_perdidas(history_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po_hBHebfUWE"
      },
      "source": [
        "Modelo Experimental #3\n",
        "\n",
        "- entrenamiento = 70%\n",
        "- validación = 15%\n",
        "- prueba = 15%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTlqUIVjfY4R"
      },
      "source": [
        "# Definimos el conjunto de entrenamiento\n",
        "me3_pares_entrenamiento = pares_aleatorizados[:porcentaje_70]\n",
        "me3_labels_entrenamiento = labels_aleatorizados[:porcentaje_70]\n",
        "\n",
        "# Definimos el conjunto de validación\n",
        "me3_pares_validacion = pares_aleatorizados[porcentaje_70:porcentaje_70+porcentaje_15]\n",
        "me3_labels_validacion = labels_aleatorizados[porcentaje_70:porcentaje_70+porcentaje_15]\n",
        "\n",
        "# Definimos el conjunto de prueba\n",
        "me3_pares_prueba = pares_aleatorizados[porcentaje_70+porcentaje_15:]\n",
        "me3_labels_prueba = labels_aleatorizados[porcentaje_70+porcentaje_15:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM5an5q6fbzJ"
      },
      "source": [
        "def build_siamese_model_3(input_shape):\n",
        "  model = tf.keras.Sequential([\n",
        "                                tf.keras.layers.Conv2D(filters=64,\n",
        "                                                        kernel_size=10,\n",
        "                                                        activation='relu',\n",
        "                                                        padding='same',\n",
        "                                                        input_shape=input_shape),\n",
        "                                tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "                                tf.keras.layers.Conv2D(filters=128,\n",
        "                                                        kernel_size=7,\n",
        "                                                        activation='relu',\n",
        "                                                        padding='same'),\n",
        "                                tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "                                tf.keras.layers.Conv2D(filters=128,\n",
        "                                                        kernel_size=4,\n",
        "                                                        activation='relu',\n",
        "                                                        padding='same'),\n",
        "                                tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "                                tf.keras.layers.Conv2D(filters=256,\n",
        "                                                        kernel_size=4,\n",
        "                                                        activation='relu',\n",
        "                                                        padding='same'),\n",
        "                                tf.keras.layers.Flatten(),\n",
        "                                tf.keras.layers.Dense(4960)\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y7pgfE6fjg4"
      },
      "source": [
        "tf.random.set_seed(100)\n",
        "\n",
        "IMG_SHAPE = (24, 16, 16)\n",
        "\n",
        "imgA = tf.keras.Input(IMG_SHAPE)\n",
        "imgB = tf.keras.Input(IMG_SHAPE)\n",
        "\n",
        "featureExtractor = build_siamese_model_3(input_shape=IMG_SHAPE)\n",
        "\n",
        "featsA = featureExtractor(imgA)\n",
        "featsB = featureExtractor(imgB)\n",
        "\n",
        "distancia = tf.keras.layers.Lambda(euclidean_distance)([featsA, featsB])\n",
        "\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(distancia)\n",
        "\n",
        "model_3 = tf.keras.Model(inputs=[imgA, imgB], outputs=outputs)\n",
        "\n",
        "model_3.compile(loss='binary_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "history_3 = model_3.fit(\n",
        "    [me3_pares_entrenamiento[:, 0], me3_pares_entrenamiento[:, 1]], me3_labels_entrenamiento[:],\n",
        "     validation_data=([me3_pares_validacion[:, 0], me3_pares_validacion[:, 1]], me3_labels_validacion[:]),\n",
        "     batch_size=32,\n",
        "     epochs=30\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSSh-rsTfn2C"
      },
      "source": [
        "graficar_curva_perdidas(history_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_hC-JWgfpDM"
      },
      "source": [
        "### Matrices de confusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVcDlP7efq_e"
      },
      "source": [
        "# Creamos una función que nos permita visualizar la matriz de confusión de los resultados\n",
        "sns.set(font_scale=1.5)\n",
        "\n",
        "def plot_conf_mat(y_test, y_preds):\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(4,4))\n",
        "    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n",
        "                     annot=True,\n",
        "                     cbar=False,\n",
        "                     fmt='g')\n",
        "    plt.xlabel('Label Reales')\n",
        "    plt.ylabel('Predicciones')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTAPmouXfzor"
      },
      "source": [
        "# Guardamos las predicciones en una variable\n",
        "predicciones_1 = model_1.predict([me1_pares_prueba[:, 0], me1_pares_prueba[:, 1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRUZ-VoQf0Y1"
      },
      "source": [
        "# Redondeamos los valores predecidos para poder compararlos con los valores reales\n",
        "for i in range(len(predicciones_1)):\n",
        "  predicciones_1[i][0] = round(predicciones_1[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IEdC2E9f2KQ"
      },
      "source": [
        "# Graficamos la matriz de confusión\n",
        "plot_conf_mat(me1_labels_prueba, predicciones_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-BZl-p6f5h7"
      },
      "source": [
        "# Guardamos las predicciones en una variable\n",
        "predicciones_2 = model_2.predict([me2_pares_prueba[:, 0], me2_pares_prueba[:, 1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zKHYE_3f7-a"
      },
      "source": [
        "# Redondeamos los valores predecidos para poder compararlos con los valores reales\n",
        "for i in range(len(predicciones_2)):\n",
        "  predicciones_2[i][0] = round(predicciones_2[i][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAqRU4bMf_mG"
      },
      "source": [
        "# Graficamos la matriz de confusión\n",
        "plot_conf_mat(me2_labels_prueba, predicciones_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lml8NRX3gAu5"
      },
      "source": [
        "# Guardamos las predicciones en una variable\n",
        "predicciones_3 = model_3.predict([me3_pares_prueba[:, 0], me3_pares_prueba[:, 1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-G-hnOxgENc"
      },
      "source": [
        "# Redondeamos los valores predecidos para poder compararlos con los valores reales\n",
        "for i in range(len(predicciones_3)):\n",
        "  predicciones_3[i][0] = round(predicciones_3[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H43-6OgagGKe"
      },
      "source": [
        "# Graficamos la matriz de confusión\n",
        "plot_conf_mat(me3_labels_prueba, predicciones_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q703MeOEgLPz"
      },
      "source": [
        "### Guardado del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pF0zCLlgNGu"
      },
      "source": [
        "# Creamos la función de guardado del modelo\n",
        "def save_model(model, suffix=None):\n",
        "  \"\"\"\n",
        "  Guarda un modelo en el directorio de modelos y agrega un string.\n",
        "  \"\"\"\n",
        "  # Creamos el pathname del directorio del modelo con el tiempo actual\n",
        "  model_dir = os.path.join('drive/MyDrive/modelos',\n",
        "                           datetime.datetime.now().strftime('%Y%m%d-%H%M%s'))\n",
        "  # Guardado con formato de modelo\n",
        "  model_path = model_dir + '-' + suffix + '.h5'\n",
        "  print(f'Guardando modelo en: {model_path}...')\n",
        "  model.save(model_path)\n",
        "  return model_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMCBmxZUgPwg"
      },
      "source": [
        "modelo_1_path = save_model(model=model_1, suffix='modelo_SNN_TurbinaJacket_fisura_pernoflojo_2')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}